---
title: "hw07-donghan2"
author: "Liu, Donghan, Donghan2"
date: "Due: Friday, April 6, 11:59 PM"
output:
  html_document:
    theme: readable
    toc: yes
  word_document:
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE,message=FALSE,warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Solution

```{r,message=FALSE,warning=FALSE}
# general
library(MASS)
library(caret)
library(tidyverse)
library(knitr)
library(kableExtra)
library(mlbench)

# specific
library(ISLR)
library(ellipse)
library(randomForest)
library(gbm)
library(glmnet)
library(rpart)
library(rpart.plot)
```


##Exercise 1 (Classifying Leukemia)

```{r,message=FALSE,warning=FALSE}
leukemia = read_csv("leukemia.csv", progress = FALSE)
y = as.factor(leukemia$class)
X = as.matrix(leukemia[, -1])
```


```{r}
#Exercise 1a
set.seed(671434599)
```

```{r}
#Exercise 1b
lasso = glmnet(X, y, family = 'binomial', alpha = 1)
ridge = glmnet(X, y, family = 'binomial', alpha = 0)

par(mfrow = c(1,2))
plot(lasso, main = 'Lasso')
plot(ridge, main = 'Ridge')
```


```{r}
#Exercise 1c
lasso_cv = cv.glmnet(X, y, family = "binomial", alpha = 1, nfolds = 5)
plot(lasso_cv)
```

```{r}
lamda_min_lasso = lasso_cv$lambda.min
lamda_1se_lasso = lasso_cv$lambda.1se

lasso_lambda = expand.grid(alpha = 1, lambda = c(lamda_min_lasso, lamda_1se_lasso))
cv_5 = trainControl(method = "cv", number = 5)
lasso1 = train(X, y, 
              method = "glmnet", 
              trControl = cv_5,
              tuneGrid = lasso_lambda)
lasso_result = lasso1$results
```


```{r}
#Exercise 1d
ridge_cv = cv.glmnet(X, y, family = 'binomial', alpha = 0, nfolds = 5)
plot(ridge_cv)
```

```{r}
lamda_min_ridge = ridge_cv$lambda.min
lamda_1se_ridge = ridge_cv$lambda.1se

ridge_lambda = expand.grid(alpha = 0, lambda = c(lamda_min_ridge, lamda_1se_ridge))
cv_5 = trainControl(method = "cv", number = 5)
ridge1 = train(X, y, 
              method = "glmnet", 
              trControl = cv_5,
              tuneGrid = ridge_lambda)
ridge_result = ridge1$results
```


```{r}
#Exercise 1e
knn = train(X,y,
            method = "knn",
            preProcess = c("scale","center"),
            trControl = cv_5)
knn_result = knn$results
```

```{r}
results = data.frame(Models = c("Lasso","Lasso","Ridge","Ridge","KNN","KNN","KNN"),Tuning_Parameter = c(sprintf("Lambda %f",lamda_min_lasso),
                     sprintf("Lambda %f",lamda_1se_lasso),sprintf("Lambda %f",lamda_min_ridge),sprintf("Lambda %f",lamda_1se_ridge),sprintf("K %f",knn_result$k[1]),sprintf("K %f",knn_result$k[2]),sprintf("K %f",knn_result$k[3])),CV_Accuracy = c(lasso_result$Accuracy[1],lasso_result$Accuracy[2],ridge_result$Accuracy[1],ridge_result$Accuracy[2],knn_result$Accuracy[1],knn_result$Accuracy[2],knn_result$Accuracy[3]), Standard_Deviation = c( lasso_result$AccuracySD[1],lasso_result$AccuracySD[2],ridge_result$AccuracySD[1],ridge_result$AccuracySD[2],knn_result$AccuracySD[1],knn_result$AccuracySD[2],knn_result$AccuracySD[3]))

kable_styling(kable(results, format = "html", digits = 4), full_width = FALSE)
```

##Exercise 2 (The Cost of College)

```{r}
set.seed(42)
index = createDataPartition(College$Outstate, p = 0.75, list = FALSE)
college_trn = College[index, ]
college_tst = College[-index, ]
```


```{r}
set.seed(671434599)
lm = train(Outstate ~ ., data = college_trn,
           method = "lm",
           trControl = cv_5)
glmnet = train(Outstate ~ ., data = college_trn,
               method = "glmnet",
               tuneLength = 10,
               trControl = cv_5)
glmnet_twoInter = train(Outstate ~ .^2, data = college_trn,
               method = "glmnet",
               tuneLength = 10,
               trControl = cv_5)
knn_mod = train(Outstate ~ ., data = college_trn,
                method = "knn",
                preProcess = c("scale","center"),
                trControl = cv_5,
                tuneLength = 20)
knn_mod_twoInter = train(Outstate ~ .^2, data = college_trn,
                method = "knn",
                preProcess = c("scale","center"),
                trControl = cv_5,
                tuneLength = 20)
rf = train(Outstate ~ ., data = college_trn,
            method = "rf",
            trControl = cv_5)
```


```{r}
calc_rmse = function(actual, model) {
  data = actual$Outstate
  predicted = predict(model,actual)
  sqrt(mean((data - predicted) ^ 2))
}
```

```{r}
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}
```


```{r}
results2 = data.frame(
Models = c("Linear Model","Elastic Net","Elastic Net With two-way Interactions","KNN","KNN with two-way Interactions","Random Forest"), 
CV_RMSE = c(get_best_result(lm)$RMSE,get_best_result(glmnet)$RMSE,get_best_result(glmnet_twoInter)$RMSE,get_best_result(knn_mod)$RMSE,get_best_result(knn_mod_twoInter)$RMSE,get_best_result(rf)$RMSE), 
Test_RMSE = c(calc_rmse(college_tst,lm),calc_rmse(college_tst,glmnet),calc_rmse(college_tst,glmnet_twoInter),calc_rmse(college_tst,knn_mod),calc_rmse(college_tst,knn_mod_twoInter), calc_rmse(college_tst,rf)))

kable_styling(kable(results2, format = "html", digits = 4), full_width = FALSE)
```


##Exercise 3 (Computation Time)

```{r}
set.seed(42)
sim_trn = mlbench.spirals(n = 2500, cycles = 1.5, sd = 0.125)
sim_trn = data.frame(sim_trn$x, class = as.factor(sim_trn$classes))
sim_tst = mlbench.spirals(n = 10000, cycles = 1.5, sd = 0.125)
sim_tst = data.frame(sim_tst$x, class = as.factor(sim_tst$classes))
```



```{r}
set.seed(671434599)
glm_cv_time = system.time({
  sim_glm_cv  = train(
    class ~ .,
    data = sim_trn,
    trControl = cv_5,
    method = "glm")
})

tree_cv_time = system.time({
  sim_tree_cv = train(
    class ~ .,
    data = sim_trn,
    trControl = cv_5,
    method = "rpart")
})
rpart.plot(sim_tree_cv$finalModel)
```

```{r}
calc_acc = function(actual, model) {
  data = actual$class
  predicted = predict(model, actual)
  mean(data == predicted)
}
```

```{r}
set.seed(671434599)

rf_grid = expand.grid(mtry = c(1, 2))
oob = trainControl(method = "oob")

rf_oob_time = system.time({
  sim_rf_oob  = train(
    class ~ .,
    data = sim_trn,
    trControl = oob,
    method = "rf",
    tuneGrid = rf_grid)
})


rf_cv_time = system.time({
  sim_rf_cv  = train(
    class ~ .,
    data = sim_trn,
    trControl = cv_5,
    method = "rf",
    tuneGrid = rf_grid)
})
```

```{r}
results3 = data.frame(
  Models = c("Logistic with CV","Tree with CV","RF with OOB","RF with CV"),
  Chosen_Tuning_Parameter = c(NA, sprintf("Cp %f",sim_tree_cv$bestTune$cp), sprintf("Mtry %f",sim_rf_oob$bestTune$mtry), sprintf("Mtry %f",sim_rf_cv$bestTune$mtry)),
  Elapsed_Tuning_Time = c(glm_cv_time["elapsed"], tree_cv_time["elapsed"], rf_oob_time["elapsed"], rf_cv_time["elapsed"]),
  Accuracy = c(sim_glm_cv$results$Accuracy, max(sim_tree_cv$results$Accuracy),max(sim_rf_oob$results$Accuracy), max(sim_rf_cv$results$Accuracy)
),
  Test_Accuracy = c(calc_acc(sim_tst, sim_glm_cv),
calc_acc(sim_tst, sim_tree_cv),
calc_acc(sim_tst, sim_rf_oob),
calc_acc(sim_tst, sim_rf_cv))
)

kable_styling(kable(results3, format = "html", digits = 4), full_width = FALSE)

```


##Exercise 4 (Predicting Baseball Salaries)

```{r}
Hitters = na.omit(Hitters)
uin = 671434599
set.seed(uin)
hit_idx = createDataPartition(Hitters$Salary, p = 0.6, list = FALSE)
hit_trn = Hitters[hit_idx,]
hit_tst = Hitters[-hit_idx,]
```


```{r}
gbm_grid = expand.grid(interaction.depth = c(1, 2),
                       n.trees = c(500, 1000, 1500),
                       shrinkage = c(0.001, 0.01, 0.1),
                       n.minobsinnode = 10)
```

```{r}
calc_rmse4 = function(actual, model) {
  data = actual$Salary
  predicted = predict(model,actual)
  sqrt(mean((data - predicted) ^ 2))
}
```

```{r}
mod_gbm_cv = train(Salary ~ ., data = hit_trn,
                   trControl = trainControl(method = "cv", number = 5), 
                   method = "gbm",
                   tuneGrid = gbm_grid,
                   verbose = FALSE)

mod_rf_oob = train(Salary ~ ., data = hit_trn,
                   trControl = trainControl(method = "oob"), 
                   method = "rf",
                   tuneGrid = expand.grid(mtry = 1:19))
mod_bagged = randomForest(Salary ~ ., data = hit_trn, mtry = 19)

results4 = data.frame(
  Models = c("Tuned Boosted Tree Model","Tuned Random Forest Model","Bagged Tree Model"),
  Resampled_RMSE = c(min(mod_gbm_cv$results$RMSE),min(mod_rf_oob$results$RMSE),mean(sqrt(mod_bagged$mse))),
  Test_RMSE = c(calc_rmse4(hit_tst, mod_gbm_cv),calc_rmse4(hit_tst, mod_rf_oob),calc_rmse4(hit_tst, mod_bagged))
)
kable_styling(kable(results4, format = "html", digits = 4), full_width = FALSE)
```


##Exercise 5 (Concept Checks)

###Leukemia

####5.1.1
```{r,eval=FALSE}
str(leukemia)
```

72 observations of  5148 variables

####5.1.2

Yes, the shape of the plot is close to U. 

####5.1.3

No, since the curve is obsiously increasing and it does not like a U shape, so there are more likely to have smaller lambda that could cause a lower error

####5.1.4

KNN does not did better than the penalized methods because KNN model has relatively lower accuracy and higher standard deviation. The difference perhaps caused by the large amound of predictors, and KNN performs not as well as penalized model.

####5.1.5

The ridge model would be chosen based on the highest accuracy and lowest standard deviation. Plus, the ridge model with higher and lower labmda value output the same values for both accuracy and sd, however, because of the high dimensions of data, the ridge with higher lambda is perfered and will cause more penalty on the model.

###College

####5.2.1

Random Forest model because of the lowest cv-RMSE and test RMSE value. 

####5.2.2

```{r}
glmnet$bestTune
```

```{r}
glmnet_twoInter$bestTune
```

Based on the choice that made by elastic net model, the alpha value is 0.1, it locates in somewhere in between but it is slightly closer to Ridge. 


####5.2.3

Yes, since scaling will help stablizing the residuals, which causes a lower error.

####5.2.4

The KNN model without two-way interaction works better. The reason is that adding more predictors into the model will likely to cause the overfitting and the data already have a high amount of data, adding interactions might also cause the issue of high dimensions to the model. 


####5.2.5

```{r}
College[rownames(College)=="University of Illinois - Urbana",]$Outstate
```

```{r}
?College
```

The R documentation says the datasets are from 1995 issue of US News and World Report


###Timing

####5.3.1

```{r}
rf_oob_time["elapsed"]
rf_cv_time["elapsed"]
```

As the elapsed time shows above, since we are using the 5-fold CV method, so the time of 5-fold cv random forest is expected to have five times than the oob random forest. However, 7.75/2.89 is not equal or close to 5. Thus, there is the difference between the OOB and CV result for the random forest similar to what I would have expected.

####5.3.2

```{r}
sim_rf_cv$bestTune
sim_rf_oob$bestTune
```

According to the best tune that chosen in the algorithm, both cv and oob random forest would perfer mtry = 1, which means they choose the same model.


####5.3.3

The model of Logistic with CV has the lowest test accuracy because the decision boundary might not be the linear

Tree with CV is doing better than the model of logistic. The reason is that it only has one single tree and the classification might not be completed deeply and thoughoutly. 

The random forest with both ood and cv have relatively high test accuracy because the statement that I mentioned above, which is they are the same model. They are better than the tree with cv is that random forest compute the accuracy by the results from using mutiple trees.

###Salary

####5.4.1

```{r}
mod_rf_oob$bestTune
```

The tuned value for mtry is 3

####5.4.2

```{r}
plot(mod_gbm_cv)
```

####5.4.3

```{r}
varImpPlot(mod_rf_oob$finalModel)
```

####5.4.4

```{r}
plot(varImp(mod_gbm_cv))
```


####5.4.5

```{r}
importance = importance(mod_rf_oob$finalModel)
importance[order(importance),][17:19]
```

The CAtBat, CRuns and CRBI are the three most important predictors in random forests.

####5.4.6

```{r}
importance_gbm = varImp(mod_gbm_cv$finalModel)
importance_gbm = as.matrix(importance_gbm)
importance_gbm[order(importance_gbm),][17:19]
```

CHmRun, CRBI, and Walks are the three most important predictors in boosted model.












