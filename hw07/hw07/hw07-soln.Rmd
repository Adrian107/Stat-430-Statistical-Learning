---
title: "Homework 07"
author: "STAT 430, Fall 2017"
date: 'Due: Friday, November 3, 11:59 PM'
urlcolor: cyan
---

You should use the `caret` package and training pipeline to complete this homework. **Any time you use the `train()` function, first run `set.seed(1337)`.**

```{r message = FALSE, warning = FALSE}
library(caret)
library(mlbench)
```

```{r, solution = TRUE, message = FALSE, warning = FALSE}
library(randomForest)
library(gbm)
library(klaR)
```

***

# Exercise 1 (Regression with `caret`)

**[10 points]** For this exercise we will train a number of regression models for the `Boston` data from the `MASS` package. Use `medv` as the response and all other variables as predictors. Use the test-train split given below. When tuning models and reporting cross-validated error, use 5-fold cross-validation.

```{r}
data(Boston, package = "MASS")
set.seed(42)
bstn_idx = createDataPartition(Boston$medv, p = 0.80, list = FALSE)
bstn_trn = Boston[bstn_idx, ]
bstn_tst = Boston[-bstn_idx, ]
```

Fit a total of five models:

- An additive linear regression
- A well tuned $k$-nearest neighbors model.
    - Do **not** scale the predictors.
    - Consider $k \in \{1, 5, 10, 15, 20, 25\}$
- Another well tuned $k$-nearest neighbors model.
    - **Do** scale the predictors.
    - Consider $k \in \{1, 5, 10, 15, 20, 25\}$
- A random forest
    - Use the default tuning parameters chosen by `caret`
- A boosted tree model
    - Use the provided tuning grid below

```{r}
gbm_grid = expand.grid(interaction.depth = c(1, 2, 3),
                       n.trees = (1:20) * 100,
                       shrinkage = c(0.1, 0.3),
                       n.minobsinnode = 20)
```

Provide plots of error versus tuning parameters for the two $k$-nearest neighbors models and the boosted tree model. Also provide a table that summarizes the cross-validated and test RMSE for each of the five  (tuned) models.

**Solution:**

```{r, solution = TRUE}
set.seed(1337)
bstn_lm_mod = train(
  medv ~ .,
  data = bstn_trn,
  trControl = trainControl(method = "cv", number = 5),
  method = "lm"
)
```

```{r, solution = TRUE}
set.seed(1337)
bstn_knnu_mod = train(
  medv ~ .,
  data = bstn_trn,
  trControl = trainControl(method = "cv", number = 5),
  method = "knn",
  tuneGrid = expand.grid(k = c(1, 5, 10, 15, 20, 25))
)
```

```{r, solution = TRUE}
set.seed(1337)
bstn_knns_mod = train(
  medv ~ .,
  data = bstn_trn,
  trControl = trainControl(method = "cv", number = 5),
  preProcess = c("center", "scale"),
  method = "knn",
  tuneGrid = expand.grid(k = c(1, 5, 10, 15, 20, 25))
)
```

```{r, solution = TRUE}
set.seed(1337)
bstn_rf_mod = train(
  medv ~ .,
  data = bstn_trn,
  trControl = trainControl(method = "cv", number = 5),
  method = "rf"
)
```

```{r, solution = TRUE}
set.seed(1337)
bstn_gbm_mod = train(
  medv ~ .,
  data = bstn_trn,
  trControl = trainControl(method = "cv", number = 5),
  method = "gbm",
  tuneGrid = gbm_grid,
  verbose = FALSE
)
```

```{r, solution = TRUE, echo = FALSE}
plot(bstn_knnu_mod, main = "KNN, Unscaled")
```

```{r, solution = TRUE, echo = FALSE}
plot(bstn_knns_mod, main = "KNN, Scaled")
```

```{r, solution = TRUE, echo = FALSE}
plot(bstn_gbm_mod, main = "Boosted Trees")
```

```{r, solution = TRUE, echo = FALSE}
calc_rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}
```

```{r, solution = TRUE, echo = FALSE}
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}
```

```{r, solution = TRUE, echo = FALSE}
reg_results = data.frame(
  method = c("Linear Regression", "KNN, Unscaled", "KNN, Scaled", 
             "Random Forest", "Boosted Trees"),
  cv = c(
    get_best_result(bstn_lm_mod)$RMSE,
    get_best_result(bstn_knnu_mod)$RMSE,
    get_best_result(bstn_knns_mod)$RMSE,
    get_best_result(bstn_rf_mod)$RMSE,
    get_best_result(bstn_gbm_mod)$RMSE
  ),
  test = c(
    calc_rmse(bstn_tst$medv, predict(bstn_lm_mod, bstn_tst)),
    calc_rmse(bstn_tst$medv, predict(bstn_knnu_mod, bstn_tst)),
    calc_rmse(bstn_tst$medv, predict(bstn_knns_mod, bstn_tst)),
    calc_rmse(bstn_tst$medv, predict(bstn_rf_mod, bstn_tst)),
    calc_rmse(bstn_tst$medv, predict(bstn_gbm_mod, bstn_tst))
  )
)
colnames(reg_results) = c("Method", "CV RMSE", "Test RMSE")
knitr::kable(reg_results, digits = 2)
```

***

# Exercise 2 (Clasification with `caret`)

**[10 points]** For this exercise we will train a number of classifiers using the training data generated below. The categorical response variable is `classes` and the remaining variables should be used as predictors. When tuning models and reporting cross-validated error, use 10-fold cross-validation.

```{r}
set.seed(42)
sim_trn = mlbench::mlbench.2dnormals(n = 750, cl = 5)
sim_trn = data.frame(
  classes = sim_trn$classes,
  sim_trn$x
)
```

```{r fig.height = 4, fig.width = 4, fig.align = "center"}
caret::featurePlot(x = sim_trn[, -1], 
            y = sim_trn$classes, 
            plot = "pairs",
            auto.key = list(columns = 2))
```

Fit a total of four models:

- LDA
- QDA
- Naive Bayes
- Regularized Discriminant Analysis (RDA)
    - Use method `rda` with `caret` which requires the `klaR` package
    - Use the default tuning grid

Provide a plot of acuracy versus tuning parameters for the RDA model. Also provide a table that summarizes the cross-validated accuracy and their standard deviations for each of the four (tuned) models.

**Solution:**

```{r, solution = TRUE}
set.seed(1337)
sim_lda_mod = train(
  classes ~ .,
  data = sim_trn,
  trControl = trainControl(method = "cv", number = 10),
  method = "lda"
)
```

```{r, solution = TRUE}
set.seed(1337)
sim_qda_mod = train(
  classes ~ .,
  data = sim_trn,
  trControl = trainControl(method = "cv", number = 10),
  method = "qda"
)
```

```{r, solution = TRUE}
set.seed(1337)
sim_nb_mod = train(
  classes ~ .,
  data = sim_trn,
  trControl = trainControl(method = "cv", number = 10),
  method = "nb"
)
```

```{r, solution = TRUE}
set.seed(1337)
sim_rda_mod = train(
  classes ~ .,
  data = sim_trn,
  trControl = trainControl(method = "cv", number = 10),
  method = "rda"
)
```

```{r, solution = TRUE, echo = FALSE}
plot(sim_rda_mod)
```


```{r, solution = TRUE, echo = FALSE}
class_results = data.frame(
    method = c("LDA", "QDA", "Naive Bayes", "RDA"),
  cv = c(
    get_best_result(sim_lda_mod)$Accuracy,
    get_best_result(sim_qda_mod)$Accuracy,
    get_best_result(sim_nb_mod)$Accuracy,
    get_best_result(sim_rda_mod)$Accuracy
  ),
  sd = c(
    get_best_result(sim_lda_mod)$AccuracySD,
    get_best_result(sim_qda_mod)$AccuracySD,
    get_best_result(sim_nb_mod)$AccuracySD,
    get_best_result(sim_rda_mod)$AccuracySD
  )
)
colnames(class_results) = c("Method", "CV Acc", "SD CV Acc")
knitr::kable(class_results, digits = 4)
```


***

# Exercise 3 (Concept Checks)

**[1 point each]** Answer the following questions based on your results from the three exercises. 

## Regression

**(a)** What value of $k$ is chosen for KNN without predictor scaling?

```{r, solution = TRUE}
bstn_knnu_mod$bestTune$k
```


**(b)** What value of $k$ is chosen for KNN **with** predictor scaling?

```{r, solution = TRUE}
bstn_knns_mod$bestTune$k
```

**(c)** What are the values of the tuning parameters chosen for the boosted tree model?

```{r, solution = TRUE}
bstn_gbm_mod$bestTune
```

**(d)** Which method achieves the lowest cross-validated error?

```{r, solution = TRUE}
reg_results[reg_results$`CV RMSE` == min(reg_results$`CV RMSE`), ]
```

**(e)** Which method achieves the lowest test error?

```{r, solution = TRUE}
reg_results[reg_results$`Test RMSE` == min(reg_results$`Test RMSE`), ]
```

## Classification

**(f)** What are the values of the tuning parameters chosen for the RDA model?

```{r, solution = TRUE}
sim_rda_mod$bestTune
```

**(g)** Based on the scatterplot, which method, LDA or QDA, do you think is *more* appropriate? Explain.

LDA. The covariance seems to be the same in each class.

**(h)** Based on the scatterplot, which method, QDA or Naive Bayes, do you think is *more* appropriate? Explain.

Naive Bayes. The predictors seem to be independent in each class.

**(i)** Which model achieves the best cross-validated accuracy?

```{r, solution = TRUE}
class_results[class_results$`CV Acc` == max(class_results$`CV Acc`), ]
```

**(j)** Do you believe the model in **(i)** is the model that should be chosen? Explain.

```{r, solution = TRUE}
rda_res = class_results[class_results$`CV Acc` == max(class_results$`CV Acc`), ]
rda_res$`CV Acc` + rda_res$`SD CV Acc`
```

No. The results of all the other model are within one SE. We should pick a less complex model, perhpas LDA.
